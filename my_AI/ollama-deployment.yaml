apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  labels:
    app: my-AI
spec:
  ports:
    - port: 11434
  selector:
    app: my-AI
    tier: backend
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-deployment
  labels:
    app: my-AI
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-AI
      tier: backend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: my-AI
        tier: backend
    spec:
      containers:
        - image: ollama/ollama:latest
          name: ollama
          lifecycle:
            postStart:
              exec:
                command:
                  - "/bin/bash"
                  - "-c"
                  - >
                    apt -y update && apt -y install curl &&
                    curl -X POST http://localhost:11434/api/pull -H "Content-Type: application/json" -d "{\"name\": \"gemma3:4b\"}" &&
                    curl -X POST http://localhost:11434/api/pull -H "Content-Type: application/json" -d "{\"name\": \"gpt-oss:20b\"}" &&
                    curl -X POST http://localhost:11434/api/pull -H "Content-Type: application/json" -d "{\"name\": \"kun432/cl-nagoya-ruri-base\"}" &&
                    export OLLAMA_NUM_THREADS=8 &&
                    export OLLAMA_MAX_LOADED_MODELS=3 &&
                    curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d "{\"model\": \"gemma3:4b\",\"keep_alive\": -1}"
          ports:
            - containerPort: 11434
          volumeMounts:
            - name: ollama-persistent-storage
              mountPath: /root/.ollama
          resources:
            limits:
              cpu: "12" #"14"
              memory: 32Gi
            requests:
              cpu: "8" #"12"
              memory: 20Gi #16Gi
      volumes:
        - name: ollama-persistent-storage
          persistentVolumeClaim:
            claimName: ollama-pv-claim
